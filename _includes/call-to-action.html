<section class="bg-primary" style="background-color: #00162b" id="about">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">WELCOME</h2>
                <hr class="primary">
<!--                <p class="text-faded">-->
                    <p>Welcome to the <strong>1<sup>st</sup> IEEE Workshop on Multimodal Content Moderation (MMCM)</strong> being held in conjunction with <a href="https://cvpr2023.thecvf.com/" style="color: #f0ed40">CVPR 2023</a></p>
                    <!--                <a href="#" class="btn btn-default btn-xl">Get Started!</a>-->
                    <p class="bg-gray" align="justify">Content moderation (CM) is a rapidly growing need in todayâ€™s world, with a high societal impact, where automated CM systems can discover discrimination, violent acts, hate/toxicity, and much more, on a variety of signals (visual, text/OCR, speech, audio, language, generated content, etc.).   Leaving or providing unsafe content on social platforms and devices can cause a variety of harmful consequences, including brand damage to institutions and public figures, erosion of trust in science and government, marginalization of minorities, geo-political conflicts, suicidal thoughts and more. Besides user-generated content, content generated by powerful AI models such as DALL-E and GPT present additional challenges to CM systems.</p>
                    <p class="bg-gray" align="justify">With the prevalence of multimedia social networking and online gaming, the problem of sensitive content detection and moderation is by nature multimodal. Moreover, content moderation is contextual and culturally multifaceted, for example, different cultures have different conventions about gestures. This requires CM approach to be not only multimodal, but also context aware and culturally sensitive.</p>

            </div>
        </div>
    </div>
</section>
