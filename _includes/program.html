<section class="bg-gray" id="program">

    <div class="container">

        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">PROGRAM</h2>
                <hr class="primary">
<!--                TBD-->
            </div>
        </div>
        <div class="col-lg-12 text-center">
            <h4><b>Venue: Room Arch 304, Seattle Convention Center</b></h4>
            <br>
        </div>
        <div class="row no-gutter">
            <table style="width: 100%">
                <tbody>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <th class="th-1-Width" style="width:12%">Time (PST)</th>
                        <th class="th-1-Width" style="width:13%">Event</th>
                        <th class="th-1-Width" style="width:48%">Title</th>
                        <th class="th-1-Width" style="width:27%">Speaker(s)</th>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>08:30 - 08:45</td>
                        <td></td>
                        <td>Opening Remarks and Logistics for the Day</td>
                        <td><a class="page-scroll" href="#O1">Mei Chen, Microsoft</a></td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>08:45 - 09:15</td>
                        <td>Invited Talk</td>
                        <td><b><a class="page-scroll" href="#talk_1">Leveraging AI Filters for Mitigating Viewer Impact from Disturbing Imagery</a></b></td>
                        <td>
                            <a class="page-scroll" href="#S6_info">Symeon Papadopoulos, ITI</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>09:15 - 09:45</td>
                        <td>Invited Talk</td>
                        <td><b><a class="page-scroll" href="#talk_2">Human Moderation in an Evolving Web Landscape: Wellness Gaps and Needs</a></b></td>
                        <td>
                            <a class="page-scroll" href="#S5_info">Marlyn Thomas Savio, TaskUs</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>09:45 - 10:15</td>
                        <td>Invited Talk</td>
                        <td><b><a class="page-scroll" href="#talk_3">Content Safeguarding in the Generative AI Era</a></b></td>
                        <td>
                            <a class="page-scroll" href="#S5_info">Paul Rad, UTSA</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>10:15 - 10:30</td>
                        <td>Coffee Break</td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>10:30 - 11:20</td>
                        <td>Invited Talk</td>
                        <td><b><a class="page-scroll" href="#talk_4">Do moderators dream of AI support? Understanding what moderators really need through community-engaged research</a></b></td>
                        <td>
                            <a class="page-scroll" href="#S2_info">Sarah Gilbert, Cornell University</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>11:20 - 12:00</td>
                        <td>Panel Discussion</td>
                        <td><b>Culture, Context, Metrics, Transparency in Responsible AI</b></td>
                        <td>
                            <a class="page-scroll" href="#S2_info">Sarah Gilbert, Cornell University</a><br>
                            <a class="page-scroll" href="#S5_info">Paul Rad, UTSA</a><br>
                            <a class="page-scroll" href="#S6_info">Symeon Papadopoulos, ITI</a><br>
                            <a class="page-scroll" href="#S3_info">Xiang Hao, Amazon</a><br>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>12:00 - 13:30</td>
                        <td>Lunch Break</td>
                        <td>
                        </td>
                        <td>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>13:30 - 14:15</td>
                        <td>Invited Talk</td>
                        <td><b><a class="page-scroll" href="#talk_5">Ephemeral Chat Moderation: Understanding Invisible Harms</a></b></td>
                        <td>
                            <a class="page-scroll" href="#S1_info">Mike Pappas, Modulate AI</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>14:15 - 14:45</td>
                        <td>Invited Talk</td>
                        <td style="padding-right: 2px">
                            <b><a class="page-scroll" href="#talk_6">Enhancing Visual Content Safety: Multimodal Approaches for Dataset Curation and Model Safeguarding</a></b>
                        </td>
                        <td>
                            <a class="page-scroll" href="#S7_info">Manuel Black, TU Darmstadt</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>14:45 – 15:15</td>
                        <td>Invited Talk</td>
                        <td style="padding-right: 2px">
                            <b><a class="page-scroll" href="#talk_7">Multimodal Deception Detection using Automatically Extracted Acoustic, Visual, and Lexical Features</a></b>
                        </td>
                        <td>
                            <a class="page-scroll" href="#S8_info">Lin Ai, Columbia University</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>15:15 - 15:45</td>
                        <td>Coffee Break</td>
                        <td> </td>
                        <td> </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>15:45 - 16:45</td>
                        <td>Invited Talk</td>
                        <td><b><a class="page-scroll" href="#talk_8">Fair and Inclusive Multimodal Generations</a></b></td>
                        <td>
                            <a class="page-scroll" href="#S4_info">Susanna Ricco, Google DeepMind</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>16:45 - 17:00</td>
                        <td>Accepted Paper</td>
                        <td><b><a class="page-scroll" href="#talk_9">An End-to-End Vision Transformer Approach for Image Copy Detection</a></b></td>
                        <td>Jiahe Steven Lee, Mong Li Lee, Wynne Hsu<br>National University of Singapore</td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>17:00 - 17:40</td>
                        <td>Panel Discussion</td>
                        <td><b>Multimodal Generation,  Moderation, Evaluation</b></td>
                        <td>
                            <a class="page-scroll" href="#S4_info">Susanna Ricco, Google DeepMind</a><br>
                            <a class="page-scroll" href="#S1_info">Mike Pappas, Modulate AI</a><br>
                            <a class="page-scroll" href="#S7_info">Manuel Black, TU Darmstadt</a><br>
                            <a class="page-scroll" href="#S8_info">Kaustav Kundu, Amazon</a><br>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>17:40 - 17:45</td>
                        <td></td>
                        <td>Closing Remarks</td>
                        <td><a class="page-scroll" href="#O1">Mei Chen, Microsoft</a></td>
                    </tr>

                </tbody>
            </table>
        </div>
        <br>
        <br>
        <div id="talk_1" class="row no-gutter">
            <h4>Leveraging AI Filters for Mitigating Viewer Impact from Disturbing Imagery</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Symeon Papadopoulos, ITI</b><br><br>
                <b>Abstract:</b> Exposure to disturbing imagery can significantly impact individuals, especially professionals who encounter such content as part of their work. This talk will summarize some of our insights from our team's recent research into leveraging AI for reducing the impact of such imagery. The talk will first briefly present how large public datasets that are seemingly innocuous could be used for creating improved AI-based content moderation models. Then, it will present the findings from a user study, involving 107 participants, predominantly journalists and human rights investigators, that explores the capability of Artificial Intelligence (AI)-based image filters to potentially mitigate the emotional impact of viewing such disturbing content.
            </div>
        </div>
        <div id="talk_2" class="row no-gutter">
            <h4>Human Moderation in an Evolving Web Landscape: Wellness Gaps and Needs</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Marlyn Thomas Savio, TaskUs</b><br><br>
                <b>Abstract:</b> Human-led content moderation, which involves trained reviewers assessing user-generated digital content to ensure compliance with a platform’s policies, has become indispensable for offering a safe and authentic experience to web users. Given moderators’ exposure to potentially traumatic content, efforts are growing to provide psychosocial interventions and support tools that can mitigate moderators’ distress. There is a need for continual empathy and innovation to keep pace with emerging challenges (e.g., disinformation, dark AI, virtual reality) that pose newer risks for the moderator population. This talk will briefly trace the professionalization of content moderation, and how engineers can help build robust systems that enable moderators to thrive and serve. Various concerns articulated by moderators that go beyond egregious content and seek technological solutions will be discussed. The presentation will call for ideating and building towards seamless human+tech workflows that can address complexity in scalable moderation.
            </div>
        </div>
        <div id="talk_3" class="row no-gutter">
            <h4>Content Safeguarding in the Generative AI Era</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Paul Rad, UTSA</b><br><br>
                <b>Abstract:</b> Online platforms are increasingly being exploited by malicious actors to disseminate unsafe and harmful content. In response, major platforms employ artificial intelligence (AI) and human moderation to obfuscate such content, ensuring user safety. Two critical needs for effectively obfuscating unsafe content are: 1) verifying the authenticity of the content, 2) providing an accurate rationale for the obfuscation, and 3) obfuscating the sensitive regions while preserving the safe regions.
In this talk, we address these challenges within the context of the generative AI era. We first emphasize the importance of verifying the authenticity of content to distinguish between genuine and manipulated media. We then introduce a Visual Reasoning Language Model (VLM) conditioned on pre-trained unsafe image classifiers to provide accurate rationales grounded in specific attributes of unsafe content. Additionally, we propose a counterfactual explanation algorithm that minimally identifies and obfuscates unsafe regions. This is achieved by utilizing an unsafe image classifier attribution matrix to guide optimal subregion segmentation, followed by an informed greedy search to determine the minimum number of subregions required to modify the classifier’s output based on the attribution score. Our approach ensures that unsafe content is effectively obfuscated while maintaining the integrity of safe content, advancing the field of content safeguarding in generative AI.
            </div>
        </div>
        <div id="talk_4" class="row no-gutter">
            <h4>Do moderators dream of AI support? Understanding what moderators really need through community-engaged research</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Sarah Gilbert, Cornell University</b><br><br>
                <b>Abstract:</b> Content moderation is central to the success of online platforms; however, getting it right is hard. While restrictive moderation stymies participation that disproportionately affects at-risk and marginalized populations, overly permissive moderation enables extremism and harassment. Negotiating this delicate balance is a daily task undertaken by millions of volunteers around the world and across platforms and modalities. However, in doing so these moderators suffer from burnout and exhaustion from repeated exposure to toxic content, stressful decision-making, and abusive users.<br>Advances in generative AI may present promising solutions to human labor and fairness issues in content moderation. However, without understanding the work moderators do, such interventions risk replicating or even exacerbating the issues they are intended to solve. By presenting a behind-the-scenes look into the challenges volunteer moderators face, I’ll share some promising avenues for advancing human-in-the-loop, AI-assisted moderation support, highlight where human labor is needed, and why community-centered approaches to research and design are a must for everyone working on moderation issues.
            </div>
        </div>
        <div id="talk_5" class="row no-gutter">
            <h4>Ephemeral Chat Moderation: Understanding Invisible Harms</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Mike Pappas, Modulate AI</b><br><br>
                <b>Abstract:</b> Content moderation tends to focus on posts, clips, and other content which is long-lived and seen by many people, as in traditional social media applications. But some of the most personal and poignant harms happen in live chats - especially voice chats - which are ephemeral and so much more difficult to moderate. In this talk, Mike Pappas (CEO of Modulate, which moderates ephemeral chats for Call of Duty, Grand Theft Auto Online, and other top platforms) will discuss where intuitions about traditional content moderation break down for ephemeral chat - and what true best practices for live chat look like.
            </div>
        </div>
        <div id="talk_6" class="row no-gutter">
            <h4>Enhancing Visual Content Safety: Multimodal Approaches for Dataset Curation and Model Safeguarding</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Manuel Black, TU Darmstadt</b><br><br>
                <b>Abstract:</b> As the landscape of generative AI evolves, addressing the safety of visual content has never been more critical.
In this talk, we consider two aspects crucial to current visual content moderation challenges.
First, we look into generative text-to-image models and approaches to automatically benchmark their susceptibility to creating unsafe images.
Specifically, our widely adopted image generation test bed—inappropriate image prompts (I2P)—contains dedicated, real-world text prompts that elicit the generation concepts such as nudity and violence.
Secondly, we will discuss how to leverage advances in vision-language models (VLMs) to perform comprehensive and flexible safety assessments at scale.
Our LlavaGuard family of safeguard models is easily customizable to varying safety policies.
Each assessment includes a safety rating, information on violated safety categories, and an in-depth textual rationale.
For example, LlavaGuard can be used for dataset curation, model safeguarding, or generative model evaluation in combination with I2P.
            </div>
        </div>
        <div id="talk_7" class="row no-gutter">
            <h4>Multimodal Deception Detection using Automatically Extracted Acoustic, Visual, and Lexical Features</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Lin Ai, Columbia University</b><br><br>
                <b>Abstract:</b> Deception detection in conversational dialogue has attracted much attention in recent years. Existing methods rely heavily on human-labeled annotations, which are costly and potentially inaccurate. In this work, we present an automated system that utilizes multimodal features for conversational deception detection without human annotations. We study the predictive power of different modalities and combine them for better performance. We use openSMILE to extract acoustic features after applying noise reduction techniques to the original audio. Facial landmark features are extracted from the visual modality. We experiment with training facial expression detectors and applying Fisher Vectors to encode sequences of facial landmarks of varying lengths. Linguistic features are extracted from automatic transcriptions of the data. We examine the performance of these methods on the Box of Lies dataset of deception game videos, achieving 73% accuracy using features from all modalities. This result is significantly better than previous results on this corpus, which relied on manual annotations, and also better than human performance.
            </div>
        </div>
        <div id="talk_8" class="row no-gutter">
            <h4>Fair and Inclusive Multimodal Generations</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Suzanna Ricco, Google DeepMind</b><br><br>
                <b>Abstract:</b> In the age of Generative AI, technology is responsible for not just finding and serving content, but also for creating the content in the first place. Defining good results is notoriously difficult. This talk introduces our research in inclusive generative models, focusing on avoiding results that reinforce harmful stereotypes. We cover both image generation and analysis tasks such as captioning and VQA, and discuss how we navigate the challenges of translating broad ethical, and necessarily subjective, goals into concrete, actionable objectives.
            </div>
        </div>
        <div id="talk_9" class="row no-gutter">
            <h4>An End-to-End Vision Transformer Approach for Image Copy Detection</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Jiahe Steven Lee, Mong Li Lee, Wynne Hsu (National University of Singapore)</b><br><br>
                <b>Abstract:</b> Image copy detection is one of the pivotal tools to safeguard online information integrity. The challenge lies in determining whether a query image is an edited copy, which necessitates the identification of candidate source images through a retrieval process. The process requires discriminative features comprising of both global descriptors that are designed to be augmentation-invariant and local descriptors that can capture salient foreground objects to assess whether a query image is an edited copy of some source reference image. This work describes an end-to-end solution that leverage a Vision Transformer model to learn such discriminative features and perform implicit matching between the query image and the reference image.
            </div>
        </div>

    </div>
</section>
