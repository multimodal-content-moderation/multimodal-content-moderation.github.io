<section class="bg-gray" id="program">

    <div class="container">

        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">PROGRAM</h2>
                <hr class="primary">
            </div>
        </div>

        <div class="row no-gutter">
            <table style="width: 100%">
                <tbody>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <th class="th-1-Width" style="width:15%">Time (PST)</th>
                        <th class="th-1-Width" style="width:15%">Event</th>
                        <th class="th-1-Width" style="width:48%">Title</th>
                        <th class="th-1-Width" style="width:22%">Speaker(s)</th>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>08:30 - 08:45</td>
                        <td></td>
                        <td>Opening Remarks and Logistics for the Day</td>
                        <td><a class="page-scroll" href="#O1">Mei Chen, Microsoft</a></td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>08:45 - 09:15</td>
                        <td>Talk</td>
                        <td><b><a class="page-scroll" href="#talk_1">Red teaming Generative AI Systems</a></b></td>
                        <td>
                            <a class="page-scroll" href="#S1_info">Lama Ahmad, OpenAI</a><br>
                            <a class="page-scroll" href="#S7_info">Pamela Mishkin, OpenAI</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>09:15 - 09:45</td>
                        <td>Talk</td>
                        <td><b><a class="page-scroll" href="#talk_2">Fact Checking 101</a></b></td>
                        <td>
                            <a class="page-scroll" href="#S2_info">Mevan Babakar, Google</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>09:45 - 10:15</td>
                        <td>Talk</td>
                        <td><b><a class="page-scroll" href="#talk_3">Content Moderation: Two Histories and Three Emerging Problems</a></b></td>
                        <td>
                            <a class="page-scroll" href="#S5_info">Tarleton Gillespie, Microsoft</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>10:15 - 10:30</td>
                        <td>Coffee Break</td>
                        <td></td>
                        <td></td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>10:30 -11:00</td>
                        <td>Talk</td>
                        <td><b><a class="page-scroll" href="#S11_info">Bias, Causality and Generative AI</a></b></td>
                        <td>
                            <a class="page-scroll" href="#S11_info">Pietro Perona, Amazon</a>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>11:00 - 11:45</td>
                        <td>Panel Discussion</td>
                        <td><b>Policy, Social Impact, Trust & Safety</b></td>
                        <td>
                            <a class="page-scroll" href="#S1_info">Lama Ahmad, OpenAI</a><br>
                            <a class="page-scroll" href="#S7_info">Pamela Mishkin, OpenAI</a><br>
                            <a class="page-scroll" href="#S2_info">Mevan Babakar, Google</a><br>
                            <a class="page-scroll" href="#S2_info">Ren√©e DiResta, Stanford University</a><br>
                            <a class="page-scroll" href="#S5_info">Tarleton Gillespie, Microsoft</a><br>
                            <a class="page-scroll" href="#S11_info">Pietro Perona, Amazon</a><br>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>11:45 - 13:00</td>
                        <td>Lunch Break</td>
                        <td>
                        </td>
                        <td>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>13:00 - 13:30</td>
                        <td>Talk</td>
                        <td style="padding-right: 2px"><b><a class="page-scroll" href="#talk_4">Generative Media Unleashed: Advancing Media Generation with Safety in Mind</a></b>
                        </td>
                        <td>
                            <a class="page-scroll" href="#S9_info">Mohammad Norouzi, Stealth Startup</a>
                        </td>
                    </tr>

                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>13:30 - 14:00</td>
                        <td>Talk</td>
                        <td><b><a class="page-scroll" href="#talk_5">Data Collection for Content Moderation</a></b></td>
                        <td><a class="page-scroll" href="#S6_info">Dmitriy Karpman, Hive AI</a></td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>14:00 - 14:15</td>
                        <td>Accepted Paper</td>
                        <td style="padding-right: 2px"><b><a class="page-scroll" href="#talk_6">CrisisHateMM: Multimodal Analysis of Directed and
                            Undirected Hate Speech in Text-Embedded Images from Russia-Ukraine Conflict</a></b>
                        </td>
                        <td>Surendrabikram Thapa</td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>14:15 - 14:30</td>
                        <td>Accepted Paper</td>
                        <td><b><a class="page-scroll" href="#talk_7">Prioritised Moderation for Online Advertising</a></b></td>
                        <td>Phanideep Gampa</td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>14:30 - 15:00</td>
                        <td>Talk</td>
                        <td style="padding-right: 2px"><b><a class="page-scroll" href="#talk_8">Understanding Health Risks for Content Moderators and Opportunities to Help</a></b>
                        </td>
                        <td><a class="page-scroll" href="#S7_info">Matt Lease, UT Austin</a></td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>15:00 - 15:30</td>
                        <td>Coffee Break</td>
                        <td> </td>
                        <td> </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>15:30 - 16:00</td>
                        <td>Talk</td>
                        <td style="padding-right: 2px"><b><a class="page-scroll" href="#talk_9">Building end-to-end content moderation pipelines in the real world</a></b>
                        </td>
                        <td><a class="page-scroll" href="#S10_info">Todor Markov, Open AI</a></td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>16:00 - 16:30</td>
                        <td>Talk</td>
                        <td><b><a class="page-scroll" href="#talk_10">Disrupting Disinformation</a></b>
                        </td>
                        <td><a class="page-scroll" href="#S4_info">Hany Farid, UC Berkeley</a></td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>16:30 - 16:40</td>
                        <td>Work-in-Progress Spotlight</td>
                        <td><b><a class="page-scroll" href="#talk_11">Safety and Fairness for Content Moderation in Generative Models</a></b>
                        </td>
                        <td>
                            <a target="_blank" href="https://www.linkedin.com/in/sarah-laszlo-284886114">Sarah Laszlo, Google</a><br>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>16:40 - 17:25</td>
                        <td>Panel Discussion</td>
                        <td><b>Technology & Approach</b></td>
                        <td>
                            <a class="page-scroll" href="#S9_info">Mohammad Norouzi, Stealth Startup</a><br>
                            <a class="page-scroll" href="#S6_info">Dmitriy Karpman, Hive AI</a><br>
                            <a class="page-scroll" href="#S7_info">Matt Lease, UT Austin</a><br>
                            <a class="page-scroll" href="#S10_info">Todor Markov, Open AI</a><br>
                            <a class="page-scroll" href="#S4_info">Hany Farid, UC Berkeley</a><br>
                        </td>
                    </tr>
                    <tr style="border-bottom: 1px solid black; height: 30px">
                        <td>17:25 - 17:30</td>
                        <td></td>
                        <td>Closing Remarks</td>
                        <td><a class="page-scroll" href="#O1">Mei Chen, Microsoft</a></td>
                    </tr>

                </tbody>
            </table>
        </div>
        <br>
        <br>
        <div id="talk_1" class="row no-gutter">
            <h4>Red teaming Generative AI Systems</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Lama Ahmad, OpenAI and Pamela Mishkin, OpenAI</b><br><br>
                <b>Abstract:</b> As generative AI systems continue to evolve, it is crucial to rigorously evaluate their robustness, safety, and potential for misuse. In this talk, we will explore the application of red teaming methodologies to assess the vulnerabilities and limitations of these cutting-edge technologies. By simulating adversarial attacks and examining system responses, we aim to uncover latent risks and propose effective countermeasures to ensure the responsible deployment of generative AI systems in new domains and modalities.
            </div>
        </div>

        <div id="talk_2" class="row no-gutter">
            <h4>Fact Checking 101</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Mevan Babakar, Google</b><br><br>
                <b>Abstract:</b> In this talk Mevan will be taking a deep dive into the world of fact checks, and fact checking. Together we'll be exploring the real-world context: How many fact checkers are out there? How are they organised? How do they fit into the information ecosystem? We'll be doing a deep dive into how fact checking actually works on the ground: Is it an effective intervention? Does it change minds? How are fact checks actually made? And we'll be ending on what the challenges are in the modern day with respect to specific examples of mis/disinformation, GenAI and global data infrastructure. Together we'll explore the opportunities and limitations, and how these will affect the future of information credibility around the world.
            </div>
        </div>

        <div id="talk_3" class="row no-gutter">
            <h4>Content Moderation: Two Histories and Three Emerging Problems</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Tarleton Gillespie, Microsoft</b><br><br>
                <b>Abstract:</b> The technical challenges of identifying toxic content are so immense, they can often eclipse the fact that identification is just one element of ‚Äòcontent moderation‚Äô as a much broader sociotechnical practice. Considering the broader historical context of content moderation helps to explain why moderation is so difficult, identify why good technical solutions don‚Äôt always make good social or political ones, and reframe what problems we‚Äôre even trying to solve. I will close by highlighting three problems that I hope will open provocative and challenging questions for those working on moderation as a technical problem.
            </div>
        </div>

        <div id="talk_4" class="row no-gutter">
            <h4>Generative Media Unleashed: Advancing Media Generation with Safety in Mind </h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Mohammad Norouzi, Stealth Startup</b><br><br>
                <b>Abstract:</b> This talk explores the exciting opportunities in diffusion models for image, video, and 3D generation. We'll dive into various generatgive media applications that'll transform and expand the creative economy, and help humans become more creative. At the same time, I will emphasize on the vital importance of trust and safety to ensure responsible and ethical utilization of these powerful technologies. I'll put forward a number of proposals for addressing safety, specifically in the generative media space.
            </div>
        </div>

        <div id="talk_5" class="row no-gutter">
            <h4>Data Collection for Content Moderation</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Dmitriy Karpman, Hive AI</b><br><br>
                <b>Abstract:</b> Data collection and curation is an integral, yet often overlooked component of building content moderation systems. In this presentation we'll discuss optimizing data annotation, the effects of data quality and quantity on overall model performance, techniques for identifying and alleviating biases in models, and discussing appropriate applications of synthetic data.
            </div>
        </div>

        <div id="talk_6" class="row no-gutter">
            <h4>CrisisHateMM: Multimodal Analysis of Directed and Undirected Hate Speech in Text-Embedded Images from Russia-Ukraine Conflict</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Authors</b><br><br>
                <b>Abstract:</b> Text-embedded images are frequently used on social media to convey opinions and emotions, but they can also be a medium for disseminating hate speech, propaganda, and extremist ideologies. During the Russia-Ukraine war, both sides used text-embedded images extensively to spread propaganda and hate speech. To aid in moderating such content, this paper introduces CrisisHateMM, a novel multimodal dataset of over 4,700 text-embedded images from the Russia-Ukraine conflict, annotated for hate and non-hate speech. The hate speech is annotated for directed and undirected hate speech, with directed hate speech further annotated for individual, community, and organizational targets. We benchmark the dataset using unimodal and multimodal algorithms, providing insights into the effectiveness of different approaches for detecting hate speech in text-embedded images. Our results show that multimodal approaches outperform unimodal approaches in detecting hate speech, highlighting the importance of combining visual and textual features. This work provides a valuable resource for researchers and practitioners in automated content moderation and social media analysis. The CrisisHateMM dataset and codes are made publicly available <a href="https://github.com/aabhandari/CrisisHateMM" target="_blank">here</a>.
            </div>
        </div>

        <div id="talk_7" class="row no-gutter">
            <h4>Prioritised Moderation for Online Advertising</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Authors</b><br><br>
                <b>Abstract:</b> Online advertisement industry aims to build a preference for a product over its competitors by making consumers aware of the product at internet scale. However, the ads that violate the applicable laws and location specific regulations can have serious business impact with legal implications. At the same time, customers are at risk of getting exposed to egregious ads resulting in a bad user experience. Due to the limited and costly human bandwidth, moderating ads at the industry scale is a challenging task. Typically at Amazon Advertising, we deal with ad moderation workflows where the ad distributions are skewed by non defective ads. It is desirable to increase the review time that the human moderators spend on moderating genuine defective ads. Hence prioritisation of deemed defective ads for human moderation is crucial for the effective utilisation of human bandwidth in the ad moderation workflow. To incorporate the business knowledge and to better deal with the possible overlaps between the policies, we formulate this as a policy gradient ranking algorithm with custom scalar rewards. Our extensive experiments demonstrate that these techniques show a substantial gain in number of defective ads caught against various tabular classification algorithms, resulting in effective utilisation of human moderation bandwidth.
            </div>
        </div>

        <div id="talk_8" class="row no-gutter">
            <h4>Understanding Health Risks for Content Moderators and Opportunities to Help</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Matt Lease, UT Austin</b><br><br>
                <b>Abstract:</b> Social media platforms must detect a wide variety of unacceptable user-generated images and videos. Such detection is difficult to automate due to high accuracy requirements, continually changing content, and nuanced rules for what is and is not acceptable. Consequently, platforms rely in practice on a vast and largely invisible workforce of human moderators to filter such content when automated detection falls short. However, mounting evidence suggests that exposure to disturbing content can cause lasting psychological and emotional damage to moderators. Given this, what can be done to help reduce such impacts? <br>
                My talk will discuss two works in this vein. The first involves the design of blurring interfaces for reducing moderator exposure to disturbing content whilst preserving the ability to quickly and accurately flag it. We find that interactive blurring can reduce psychological impacts on workers without sacrificing moderation accuracy or speed (see demo at http://ir.ischool.utexas.edu/CM/demo/). Following this, I describe a broader analysis of the problem space, conducted in partnership with clinical psychologists responsible for wellness measurement and intervention in commercial moderation settings. This analysis spans both social and technological approaches, reviewing current best practices and identifying important directions for future work, as well as the need for greater academic-industry collaboration
            </div>
        </div>

        <div id="talk_9" class="row no-gutter">
            <h4>Building end-to-end content moderation pipelines in the real world</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Todor Markov, Open AI</b><br><br>
                <b>Abstract:</b> In this talk, we explore a holistic approach for building a natural language classification system tailored for content moderation in real-world scenarios. We discuss the importance of crafting well-defined content taxonomies and labeling guidelines to ensure data quality, and detail the active learning pipeline developed to handle rare events effectively. We also examine various techniques used to enhance the model's robustness and prevent overfitting. This approach generalizes to diverse content taxonomies and how the resulting classifiers can outperform standard off-the-shelf models in the context of content moderation.
            </div>
        </div>

        <div id="talk_10" class="row no-gutter">
            <h4>Disrupting Disinformation</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Hany Farid, UC Berkeley</b><br><br>
                <b>Abstract:</b> We are awash in disinformation consisting of lies and conspiracies, with real-world implications ranging from horrific humans rights violations to threats to our democracy and global public health. Although the internet is vast, the peddlers of disinformation appear to be more localized. I will describe a domain-level analysis for predicting if a domain is complicit in distributing or amplifying disinformation. This process analyzes the underlying domain content and the hyperlinking connectivity between domains to predict if a domain is peddling in disinformation. These basic insights extend to an analysis of disinformation on Telegram and Twitter.
            </div>
        </div>

        <div id="talk_11" class="row no-gutter">
            <h4>Safety and Fairness for Content Moderation in Generative Models</h4>
            <div class="col-md-12" style="margin-bottom: 30px; text-align: justify">
                <b>Speakers: Sarah Laszlo, Google</b><br><br>
                <b>Abstract:</b> With significant advances in generative AI, new technologies are rapidly being deployed with generative components. Generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. Responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. Here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. We define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. We then provide a demonstration of how the defined harms can be quantified. We conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.
            </div>
        </div>




<!--        <div class="row no-gutter">-->
<!--            <table style="width: 100%;">-->
<!--                <tbody>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <th class="th-1-Width" style="width:80%">Session</th>-->
<!--                        <th>Start - End (PST)</th>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Opening Remarks and Logistics for the Day</td>-->
<!--                        <td>08:30 - 08:45</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Lama Ahmad & Pamela Mishkin, OpenAI, "Red teaming Generative AI Systems"-->
<!--                        </td>-->
<!--                        <td>08:45 - 09:15</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Mevan Babakar, Google, "Fact Checks and Fact Checking"</td>-->
<!--                        <td>09:15 - 09:45</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Tarleton Gillespie, Microsoft, "Content Moderation: Two Histories and Three-->
<!--                            Emerging Problems"-->
<!--                        </td>-->
<!--                        <td>09:45 - 10:15</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Coffee Break</td>-->
<!--                        <td>10:15 - 10:30</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Pietro Perona,azon, "Bias, Causality and Generative AI"</td>-->
<!--                        <td>10:30 -11:00</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Lama, Pamela, Mevan, Ren√©e, Tarleton, Pietro, Panel on "Policy, Social-->
<!--                            Impact, Trust & Safety"-->
<!--                        </td>-->
<!--                        <td>11:00 - 11:45</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Lunch Break</td>-->
<!--                        <td>11:45 - 01:00</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Mohammad Norouzi, Stealth Startup, "Generative Media Unleashed: Advancing-->
<!--                            Media Generation with Safety in Mind"-->
<!--                        </td>-->
<!--                        <td>01:00 - 01:30</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Dmitriy Karpman, Hive AI, "Data Collection for Content Moderation"</td>-->
<!--                        <td>01:30 - 02:00</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Accepted paper, "CrisisHateMM: Multimodal Analysis of Directed and-->
<!--                            Undirected Hate Speech in Text-Embedded Images from Russia-Ukraine Conflict"-->
<!--                        </td>-->
<!--                        <td>02:00 - 02:15</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Accepted paper, "Prioritised Moderation for Online Advertising"</td>-->
<!--                        <td>02:15 - 02:30</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Matt Lease, UT Austin, "Understanding Health Risks for Content Moderators-->
<!--                            and Opportunities to Help"-->
<!--                        </td>-->
<!--                        <td>02:30 - 03:00</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Coffee Break</td>-->
<!--                        <td>03:00 - 03:30</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Todor Markov, OpenAI, "Building end-to-end content moderation pipelines in-->
<!--                            the real world"-->
<!--                        </td>-->
<!--                        <td>03:30 - 04:00</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Hany Farid, UC Berkeley, "Disrupting Disinformation"-->
<!--                        </td>-->
<!--                        <td>04:00 - 04:30</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Work-in-Progress Spotlight, "Safety and Fairness for Content Moderation in Generative Models"-->
<!--                        </td>-->
<!--                        <td>04:30 - 04:40</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Mohammad, Dmitriy, Matt, Todor, Hany, Panel on "Technology & Approach"-->
<!--                        </td>-->
<!--                        <td>04:40 - 05:25</td>-->
<!--                    </tr>-->
<!--                    <tr style="border-bottom: 1px solid black; height: 30px">-->
<!--                        <td>Closing remarks-->
<!--                        </td>-->
<!--                        <td>05:25 - 05:30</td>-->
<!--                    </tr>-->
<!--                </tbody>-->
<!--            </table>-->

<!--        </div>-->

    </div>
</section>